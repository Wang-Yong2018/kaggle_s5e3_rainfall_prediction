---
title: "ml_workflow"
author: "WangYong"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Target
The goal of this competition is to predict rainfall for each day of the year.

ML tools: 
  tidymodels related worksflows & glm, lightgbm, ranger engine.
Evaluatioin metric:
  roc_auc, percent of rain probability.
  sample_submission.csv : 0.5. it is value is 0. I tried change it to 1, the score is 0.5 as well.
  kaggle best score is 0.87 in the begining.
  
Notice:
- from small data, data usage is as important as feature engineering. cv with repeate could slightly reduce the variance(std_err). as side effect, the metric might be improve a bit as well.
- in last stage, consider change cv =10, repeate = 5. it is computation expensive task. only do at final stage.

## librar y & load_data

### library

```{r}
library(tidyverse)
library(tidymodels)
library(finetune)
library(future)
library(purrr)
library(furrr)
library(textrecipes)
library(themis)


library(bonsai)
library(lightgbm)
library(xgboost)
library(ranger)

library(readr)
library(janitor)
library(lubridate)
```

### loading data

```{r}
data_path <- '../input/playground-series-s5e3/'
train<- 
  readr::read_csv(file.path(data_path, 'train.csv'),
                  show_col_types = FALSE)|>
  mutate(rainfall=as.factor(rainfall))
test <- 
  readr::read_csv(file.path(data_path, 'test.csv'),
                  show_col_types = FALSE)
submission <-  readr::read_csv(file.path(data_path, 'sample_submission.csv'),show_col_types = FALSE)
```

### quick skim

```{r}
train|> skimr::skim()
```

```{r}
test|> skimr::skim()
```

```{r}
submission |> skimr::skim()
```

### check if train & test is same distribution

```{r}
get_df_var<-function(df){
  df|>
    select(-any_of(c('id','rainfall')))|>
    summarize_all(var)|>
    pivot_longer(cols=everything(),
                 names_to='feature',
                 values_to='variance')

}
list(train=train, test=test)|>
  map_dfr(\(x) get_df_var(x), .id = "dataset") |>
  pivot_wider(names_from=dataset, values_from = variance)|>
  mutate(pct_change=(train-test)/train)#|>arrange(desc(abs(diff)))
```

### Finding of different distribution
there is no big change found 

## EDA

### time series EDA
```{r}
library(fpp3)

trans_tsibble <-function(){

  tmp <- bind_rows(
    list(train=train,test=test),
    .id = 'source') |>
    mutate(
      source=as.factor(source),
      day=case_when(id==1037~307+1,
                    id == 1132~37+1,
                    id ==1210~115+1,
                    id ==1132~265+1,
                    id ==1251~156+1,
                    id ==1284~189+1,
                    id == 1290~195+1,
                    id== 1312~217+1,
                    id == 1318~223+1,
                    id ==1346~251+1,
                    id == 1352~257+1,
                    id == 1367~272+1,
                    id ==  1373~278+1,
                    id ==1380~285+1,
                    id == 1382~287+1,
                    id ==1388~293+1,
                    id == 1395~300+1,
                    id == 1400~305+1,
                    id == 1403~308+1,
                    id ==1406~311+1,
                    id == 1404~309+1,
                    id == 1407~312+1,
                    id == 1409~314+1,
                    id == 1414~319+1,
                    id == 1416~321+1,
                    id ==1420 ~325+1,
                    id == 1428~333+1,
                    id ==1430~335+1,
                    id ==1438~343+1,
                    id == 1439~344+1,
                    id == 1445 ~ 350+1,
                    id == 1452 ~ 357 +1,
                    id == 1453 ~ 358+1,
                    id == 1457 ~ 362+1,
                    id == 1458 ~ 363+1,
                    id == 1459 ~ 364+1,
                    .default=day),
      year_offset = case_when( id >=0 & id <365~1,
                               id >=365 & id <730~2,
                               id >=730 & id <1095~3,
                               id >=1095 & id <1460~4,
                               id >=1460 & id <1825~5,
                               id >=1825 & id <2190~6,
                               id >=2190 & id <2555~7,
                               id >=2555 ~8))|>
    mutate(date = make_date(year = 2020 + year_offset, # create the year column for the date
                            month = 1,
                            day =1) + days(day -1) # days is offset from teh start of the year
    )
  tmp|>
    as_tsibble(index=date)
}
# 
# ts_df <- trans_tsibble()
# ts_df |> ggplot2::ggplot(aes(x=id, y=day,color=source)) + geom_point()


```
#### plot ts
```{r}
# ts_df |>as_tibble()|>
#   select(-any_of(c('id','day', 'rainfall','year_offset','date')))|>
#   group_by(source)|>
#   summarize_all(var)|>
#   pivot_longer(-source)|>
#   pivot_wider(names_from = source, values_from=value)|>
#   mutate(pct_change= (train-test)/train)
```



## coding

### 1. Data Loading and Initial Exploration ----

### 2. Feature Engineering ----

-   leave it in the preprocessing recipe

### 3. Data Splitting ----

#### augment_df

#### split/cv

```{r}
set.seed(1234)
#train <- ts_df |> as_tibble() |>filter(source=='train')|>select(-source)
#df_split <- initial_time_split(train, prop = 0.8)
df_split <- initial_split(train, prop = 0.8, strata = rainfall)
train_set <- training(df_split)
test_set <- testing(df_split)
cv_folds <- vfold_cv(train_set,
                     v = 10,
                     repeats = 1,
                     strata = rainfall)
#cv_folds <- train_set |> sliding_period(index=date, period='year')
```

### 4. Preprocessing Recipe ----

#### 4.0 v0 base_line - 
```{r}
rcp_bs_v0 <-
  recipe(rainfall ~ ., data = train_set) |>
  update_role(id, new_role='ID')|>
  #step_rm(date,year_offset)|>
  step_impute_median(all_numeric_predictors())|> 
  step_YeoJohnson(all_numeric_predictors()) |>
  # process the logical feature to factor feature
  step_bin2factor(all_logical_predictors())|>
  # proceeding the convert the character feature to factorical feature
  step_novel(all_nominal_predictors())|>
  step_unknown(all_nominal_predictors()) |>
  step_other(all_nominal_predictors())|>
  step_dummy(all_nominal_predictors(),one_hot = TRUE) |>
  step_nzv(all_predictors())|>
  #step_corr(all_numeric_predictors())|>
  step_normalize(all_numeric_predictors())|> # Scale numeric predictors
  #step_smote(rainfall,over_ratio = 1,skip = TRUE)|>
  check_missing(all_predictors())
```
#### 4.11 v11_box_cox
```{r}
rcp_v11_boxcox <-
  recipe(rainfall ~ ., data = train_set) |>
  update_role(id, new_role='ID')|>
  #step_rm(date,year_offset)|>
  step_impute_median(all_numeric_predictors())|> 
  step_mutate(
    temp_range= maxtemp - mintemp,
    heat_index= temparature+ 0.5 * (temparature- 10) * (humidity / 100),
    dew_dep= temparature - dewpoint,
    #cloud_sun_ratio = cloud / sunshine + 1,    
    #pressure_change = c(NA, diff(pressure)),
    # air_density = 1.225 , 
    # wind_power = 0.5 * air_density * windspeed**3,
    # wind_direction_cat = cut(winddirection, 
    #                          breaks=c(0, 90, 180, 270, 360), 
    #                          labels=c("North", "East", "South", "West"), 
    #                          include.lowest=TRUE),
    # cloud_humidity = cloud + humidity,
    # cloud_humidity_sunshine = cloud + humidity + sunshine,
    # cloud_x_sunshine = cloud * sunshine,
    humidity_x_sunshine = humidity * sunshine)|>
  step_novel(all_nominal_predictors())|>
  step_unknown(all_nominal_predictors()) |>
  step_other(all_nominal_predictors())|>
  step_BoxCox(all_numeric_predictors())|> # bad
  step_dummy(all_nominal_predictors(),one_hot = TRUE) |>
  step_impute_median(all_numeric_predictors())|> 
  step_normalize(all_numeric_predictors())|>
  step_zv(all_predictors())|>
  step_corr(all_predictors())|>
  check_missing(all_predictors())
```

#### 4.12 v3 
```{r}
rcp_v12 <-
  recipe(rainfall ~ ., data = train_set) |>
  update_role(id, new_role='ID')|>
  #step_rm(date,year_offset)|>
  step_impute_median(all_numeric_predictors())|> 
  step_mutate(
    cloud = cloud /100,
    sunshine = sunshine/12,
    dewpoint_averge = dewpoint -temparature,
    humidity_sig = 1 / (1 + exp(-0.2*(humidity - 80)))
    )|>
  step_rm(id,day,dewpoint,winddirection,maxtemp,mintemp)|>
  #step_novel(all_nominal_predictors())|>
 # step_unknown(all_nominal_predictors()) |>
  #step_other(all_nominal_predictors())|>
  #step_BoxCox(all_numeric_predictors())|> # bad
  #step_dummy(all_nominal_predictors(),one_hot = TRUE) |>
  #step_impute_median(all_numeric_predictors())|> 
  step_normalize(all_numeric_predictors())|>
  step_zv(all_predictors())|>
  step_corr(all_predictors())|>
  check_missing(all_predictors())
 
```
#### 4.13 v5 
```{r}
rcp_v13 <-
   recipe(rainfall ~ ., data = train_set) |>
  update_role(id, new_role='ID')|>
  step_mutate(
    cloud = cloud /100,
    sunshine = sunshine/12,
    dewpoint_averge = dewpoint -temparature,
    temp_range = maxtemp-mintemp,
    humidity_sig = 1 / (1 + exp(-0.2*(humidity - 80)))
    )|>
  step_bs(sunshine)|>
  step_lag(pressure,temparature,dewpoint_averge,temp_range, windspeed)|>
  step_impute_roll(all_numeric_predictors()) |>
  step_mutate(#month = round(day/12), 
              week= round(day/52)
              #,season=round(day/122)
              ) |>
  step_hyperbolic(winddirection,windspeed)|>
  step_rm(id,day,dewpoint,winddirection,maxtemp,mintemp)|>
  step_dummy(all_nominal_predictors(),one_hot = TRUE)|>
  step_corr(all_numeric_predictors())|>
  step_zv(all_predictors())|>
  check_missing(all_predictors())
```
    
#### 4.1 v1 base_line - bad
```{r}
rcp_bs_v1 <-
  recipe(rainfall ~ ., data = train_set) |>
  update_role(id, new_role='ID')|>
  #step_date(date, features=c('month','week'))|>
  step_impute_median(all_numeric_predictors())|> 
   step_mutate(
   temp_range= maxtemp - mintemp,
   heat_index= temparature+ 0.5 * (temparature- 10) * (humidity / 100),
   dew_dep= temparature - dewpoint,
   #cloud_sun_ratio = cloud / sunshine + 1,    
   pressure_change = c(NA, diff(pressure)),
   air_density = 1.225 , 
   wind_power = 0.5 * air_density * windspeed**3,
   wind_direction_cat = cut(winddirection, 
                            breaks=c(0, 90, 180, 270, 360), 
                            labels=c("North", "East", "South", "West"), 
                            include.lowest=TRUE),
   cloud_humidity = cloud + humidity,
   cloud_humidity_sunshine = cloud + humidity + sunshine,
   cloud_x_sunshine = cloud * sunshine,
   humidity_x_sunshine = humidity * sunshine)|>
  #step_rm(date,year_offset)|>
  step_impute_median(all_numeric_predictors())|> 
  step_YeoJohnson(all_numeric_predictors()) |>
  # process the logical feature to factor feature
  step_bin2factor(all_logical_predictors())|>
  # proceeding the convert the character feature to factorical feature
  step_novel(all_nominal_predictors())|>
  step_unknown(all_nominal_predictors()) |>
  step_other(all_nominal_predictors())|>
  step_dummy(all_nominal_predictors(),one_hot = TRUE) |>
  step_nzv(all_predictors())|>
  step_corr(all_numeric_predictors())|>
  step_normalize(all_numeric_predictors())|> # Scale numeric predictors

  check_missing(all_predictors())
```

#### 4.2 v2 base_line - 
```{r}

rcp_bs_v2 <-
  recipe(rainfall ~ ., data = train_set) |>
  update_role(id, new_role='ID')|>
  #step_rm(date,year_offset)|>
  step_impute_median(all_numeric_predictors())|> 
  step_mutate(
   temp_range= maxtemp - mintemp,
   heat_index= temparature+ 0.5 * (temparature- 10) * (humidity / 100),
   dew_dep= temparature - dewpoint,
   cloud_sun_ratio = sunshine/cloud,    
   pressure_change = c(NA,diff(pressure)),
   air_density = 1.225 , 
   wind_power = 0.5 * air_density * windspeed**3,
   # wind_direction_cat = cut(winddirection, 
   #                          breaks=c(0, 90, 180, 270, 360), 
   #                          labels=c("North", "East", "South", "West"), 
   #                          include.lowest=True),
   cloud_humidity = cloud + humidity,
   cloud_humidity_sunshine = cloud + humidity + sunshine,
   cloud_x_sunshine = cloud * sunshine,
   humidity_x_sunshine = humidity * sunshine)|>
  step_impute_median(all_numeric_predictors())|> 
  step_YeoJohnson(all_numeric_predictors()) |>
  # process the logical feature to factor feature
  step_bin2factor(all_logical_predictors())|>
  # proceeding the convert the character feature to factorical feature
  step_novel(all_nominal_predictors())|>
  step_unknown(all_nominal_predictors()) |>
  step_other(all_nominal_predictors())|>
  step_dummy(all_nominal_predictors(),one_hot = TRUE) |>
  step_nzv(all_predictors())|>
  #step_corr(all_numeric_predictors())|>
  step_normalize(all_numeric_predictors())|> # Scale numeric predictors
  #step_smote(rainfall,over_ratio = 1,skip = TRUE)|>
  check_missing(all_predictors())

```

#### 4.10 all recipes

```{r}
set.seed(1234)
library(future)
library(furrr)
selected_rcps <- list(#bs=rcp_bs_v0,
                    #v11= rcp_v11_boxcox,
                    #v12=rcp_v12,
                    v13=rcp_v13
                      #bs_wd_smote=rcp_bs_v0_wd_smote
                      #bs_v1=rcp_bs_v1,#, # is significanttly change the data , and performance is descress . give it up.
                      #bs_v2=rcp_bs_v2
                      )
plan(multisession,workers = 5)
 #selected_rcps|>map(\(rcp_item) rcp_item|>prep()|>bake(new_data=train_set)|>summary())
plan(sequential)
```

### 5. Model Specification ----

```{r}
glm_eng <- 
  logistic_reg(penalty = 0.01623777,
               mixture = 0.05) |>  # Example penalty and mixture values
  set_engine("glmnet") |>
  set_mode("classification")    # Specify classification

lgbm_eng<-
   parsnip::boost_tree(
      trees = 500, # Number of trees
      learn_rate = 0.01,
      tree_depth =5,
      loss_reduction = 0.001,
      stop_iter = 50,
      sample_size = 0.9, # Added sample_size
      #tree_depth = tune(),
      #mtry = 0.5,
      min_n = 100
   ) |>
   set_mode("classification")|>
   set_engine("lightgbm",
              #metric='roc_auc', 
              num_leaves = 20,
              counts = FALSE,
              num_threads=12,
              metric = "auc",              # 优化目标
              # reg_alpha=0.01,
              # reg_lambda = 0.5,
              verbose=1) 

rf_eng<- rand_forest( trees = 700, 
                      #mtry=100, 
                      min_n=100) |>
  set_engine("ranger",num.threads=4)|>
  set_mode("classification") 

xgb_eng<- parsnip::boost_tree( trees = 500, 
                      learn_rate = 0.01,
                      loss_reduction = 0.001,
                      sample_size = 0.8, # Added sample_size
                      #mtry=tune(),
                      min_n=70) |>
  set_engine("xgboost",num.threads=8)|>
  set_mode("classification")
#[1] "use_C5.0"             "use_cubist"           "use_earth"            "use_glmnet"           "use_kernlab_svm_poly" "use_kernlab_svm_rbf" 
#[7] "use_kknn"             "use_ranger"           "use_xgboost" 

c50_eng <- boost_tree() |>
  set_mode('classification')|>
  set_engine('C5.0')

earth_eng <-  # good model base score 0.8718
  mars() %>% 
  set_mode("classification") %>% 
  set_engine("earth") 

svm_eng <- 
  svm_rbf(
    cost = 0.001, 
    rbf_sigma = 0.01
    ) %>% 
  set_mode("classification") 

kknn_eng <- 
  nearest_neighbor(neighbors = 5, 
                   #weight_func = tune()
                   ) %>% 
  set_mode("classification") %>% 
  set_engine("kknn") 

selected_eng <- list(glm=glm_eng,
                     rf=rf_eng,
                     lgbm=lgbm_eng,
                     xgb=xgb_eng,
                     #c50=c50_eng,
                     earth= earth_eng,
                     #kknn=kknn_eng,
                     svm=svm_eng
                     
                     )

```

### 6. Workflow ----
#### set metrics
```{r}
rocauc_metrics <- metric_set(roc_auc) # main goal is roc_auc, accuracy is just for reference
```

#### simple wflow

```{r}
set.seed(1234)

simple_wf_fit <- 
  workflow() |>
  add_recipe(rcp_v13) |>
  add_model(lgbm_eng)|>
  # fit(train_set)
  fit_resamples(cv_folds,
  #  last_fit(df_split,
          control = control_resamples(verbose=TRUE),
           metrics=rocauc_metrics)
#plan(sequential)
 simple_wf_fit |> collect_metrics()
#   extract_fit_engine()|>
#   plot()

```

#### simple workflowset

```{r}
set.seed(1234)
library(future)
plan(multisession,workers = 16)
ctrl <- control_resamples(save_pred = TRUE, save_workflow = TRUE,verbose=TRUE)
wfs_result <-
  workflow_set(preproc = selected_rcps,
               models = selected_eng ) |>
  workflow_map(fn='fit_resamples',
               resamples =cv_folds,
               metrics =rocauc_metrics,
               control = ctrl
               )
wfs_result|> 
  collect_metrics()  |>
  filter(.metric=='roc_auc')|>
  select(wflow_id, model, .metric,mean, n, std_err
         )
  
plan(sequential)
```

### 7 stacking

```{r}
set.seed(1234)
library(future)
#plan(multisession,workers = 4)
combined_fit <-
  stacks::stacks()|>
  stacks::add_candidates(wfs_result)|>
  stacks::blend_predictions(penalty = 10^seq(-2, -0.1, length = 20))|>
  stacks::fit_members()

combined_fit|>
  autoplot(type = "weights")

autoplot(combined_fit)
#plan(sequential)
```

### 7. Tuning Grid ----
#### tune lgbm
```{r}
# tune_lgbm_spec <-
#   boost_tree(
#     trees =tune(),             # Number of trees (boosting rounds)
#     #mtry = tune(),              # Number of features to consider at each split
#     learn_rate = 0.01,        # Learning rate (shrinkage)
#     tree_depth = tune(),         # Maximum depth of each tree
#     min_n = tune(),            # Minimum number of data points in a node
#     sample_size= tune(),
#     loss_reduction = 0.0001      # Complexity parameter
#   ) %>%
#   set_engine("lightgbm",
#              metric='auc',
#              num_leaves=tune(),
#              num_threads=8) %>%
#   set_mode("classification")
```


```{r}
# set.seed(1234)
# 
# tune_lgbm_grid <-
#   tune_lgbm_spec %>%
#   extract_parameter_set_dials()|> 
#   update( trees = trees(range = c(700, 1500)),  # Wider range
#           #mtry = mtry(range = c(0.5, 0.9)),    # Wider range
#           #learn_rate = learn_rate(range = c(-3, -1)),  # Log scale
#           sample_size= sample_prop(range=c(0.6,0.9)),
#           tree_depth = tree_depth(range = c(6, 12)),  # Wider range
#           min_n = min_n(range = c(30, 100)),   # Wider range
#           #loss_reduction = loss_reduction(range = c(-4, -1)), # Wider range
#           num_leaves = num_leaves(range=c(31,100))            # 直接设定树数量范围
#           ) |>
#    grid_space_filling(size = 5)
# 
# ctrl <- control_race(verbose_elim = TRUE,verbose=TRUE)
# tune_res <- 
#   workflow()|>
#   add_model(tune_lgbm_spec)|>
#   add_recipe(rcp_bs_v0_wd_smote)|>
#    tune_race_anova( resamples = cv_folds, grid=tune_lgbm_grid,control=ctrl)
```


```{r}
# tune_lgbm_grid <- grid_space_filling(
#   trees(range = c(500, 1000)),  # Wider range
#   #mtry = mtry(range = c(0.5, 0.9)),    # Wider range
#   learn_rate(range = c(-3, -1)),  # Log scale
#   tree_depth(range = c(5, 12)),  # Wider range
#   min_n(range = c(10, 60)),   # Wider range
#   loss_reduction(range = c(-4, -1)), # Wider range
#   num_leaves(range=c(31,100)),
#   size = 30 # Increased number of combinations to try
# )
# 
# tune_lgbm_workflow <- 
#   workflow_set(preproc = selected_rcps,
#                models = list(lgb=tune_lgbm_spec) ) 
# 
# # Tune the model using ANOVA Race
# tune_results <-  tune_lgbm_workflow |>
#   workflow_map(fn='tune_race_anova',
#                resamples=cv_folds,
#                seed=1234,
#                grid=tune_lgbm_grid,
#                metrics=rocauc_metrics,
#                control = control_race(save_pred = TRUE, save_workflow = TRUE,verbose=TRUE) # Show progress
#                )
# tune_results|>rank_results()

```


#### tune glmnet
```{r}
# 
# glmnet_recipe <- rcp_bs_v0_wd_smote
# 
# glmnet_spec <- 
#   multinom_reg(penalty = tune(), mixture = tune()) %>% 
#   set_mode("classification") %>% 
#   set_engine("glmnet") 
# 
# glmnet_workflow <- 
#   workflow() %>% 
#   add_recipe(glmnet_recipe) %>% 
#   add_model(glmnet_spec) 
# 
# glmnet_grid <- tidyr::crossing(penalty = 10^seq(-4, -1, length.out = 20), 
#                                mixture = c(0.05, 0.2, 0.4, 0.6, 0.8, 1)) 
# 
# 
# glmnet_tune <- 
#   tune_grid(glmnet_workflow, resamples =cv_folds, grid = glmnet_grid,
#             control=control_grid(save_pred = TRUE, 
#                                  verbose = TRUE,
#                                  allow_par = F)) # Keep predictions
# 
# glmnet_tune |>show_best()
# ```
# #### tune lgbm
# ```{r}
# lgbm_recipe <- rcp_bs_v0_wd_smote 
# 
# lgbm_spec <-  
#   boost_tree(
#     trees = tune(),
#     tree_depth = tune(),
#     learn_rate = tune(),
#     mtry = tune(),
#     min_n = tune(), 
#     loss_reduction = numeric() ) |> 
#   set_engine("lightgbm",
#              max_bin=tune(),
#               # reg_lambda = tune(),  
#               # max_bin = tune(),
#               # min_sum_hessian_in_leaf = tune(),
#              #bagging_fraction = tune()
#             )|>
#   set_mode("classification")
# 
# lgbm_workflow <- 
#   workflow() |> 
#   add_recipe(lgbm_recipe) |>
#   add_model(lgbm_spec) 
# lgbm_grid <- lgbm_workflow |>
#   extract_parameter_set_dials( ) |>
#   update(
#     trees= trees(range=c(300,700)),
#     tree_depth = tree_depth(range = c(3, 8)),  # 原默认范围可能更宽，此处缩小
#     learn_rate = learn_rate(range = c(-2, -1)), # 指数范围：0.01 ~ 0.1
#     mtry = mtry(range=c(5,11)),
#     max_bin = integer(c(31,128)),
#     min_n = min_n(range = c(10, 50))
#   )|>
#   grid_space_filling(size=5)
# 
# set.seed(1234)
# lgbm_tune <-
#   tune_grid(lgbm_workflow, 
#             resamples = cv_folds, 
#             grid =lgbm_grid,
#             control=control_race(save_pred = TRUE, save_workflow = TRUE,verbose=TRUE))
# 
# lgbm_tune |> show_best()
```

### 8. Cross-Validation ----

```{r}
# combined it with step3 data splitting
```

### 9. Tuning and Evaluation ----

```{r}
# plan(multisession,workers =2)
# cars_tune_results <- cars_workflow |>
#   tune_grid(
#     resamples = cars_folds,
#     grid = cars_grid,
#     metrics = metric_set(rmse),
#      control = control_grid(save_pred = TRUE, 
#                             verbose = TRUE,
#                             allow_par = F) # Keep predictions
#   )
#  
#  # Find best parameters
#  best_params <- cars_tune_results |>
#    select_best("rmse")
# 
#  # Finalize workflow with best parameters
#  final_workflow <- cars_workflow |>
#    finalize_workflow(best_params)
```

```{r}
# Fit the final workflow to the training data
# final_lgbm_fit <- last_fit(final_workflow,cars_split )
# final_lgbm_mod <- extract_workflow(final_lgbm_fit )
# collect_metrics(final_lgmb_mod)

# plan(sequential)

```

### 10. Evaluate on Test Set ----

```{r}
combined_test_result <- 
  test_set %>%
  bind_cols(predict(combined_fit, 
                    new_data=test_set,type='prob'))
combined_test_result|>rocauc_metrics(rainfall, .pred_1,event_level = 'second')
```

### 11. Prepare Submission ----

```{r}
set.seed(1234)
library(future)
plan(multisession,workers = 12)
final_model <- combined_fit#simple_wf_fit|>extract_workflow()
#final_model <- simple_wf_fit|>extract_workflow()
final_predictions <- final_model |>
   predict(new_data = test,
           type='prob') 
plan(sequential)

 # #Handle negative predictions
 # final_predictions <- final_predictions |>
 #   mutate(.pred= ifelse(.pred< 0, 0, .pred))

 # Save submission file
 submission |>
   mutate(rainfall=final_predictions$.pred_1)|>
   readr::write_csv("submission.csv")
 zip('submission.csv.zip','submission.csv')
 
```

## kaggle submission



### score submit
```{r}
# submit latest submission.csv
system('kaggle competitions submit -c playground-series-s5e3 -f submission.csv.zip -m "fe 0310"')
Sys.sleep(15)
# get latest score 
system('kaggle competitions submissions -q -c playground-series-s5e3')
# 
# # get leader board score
# #system('kaggle competitions leaderboard -s -v -c playground-series-s4e8')
```

### notebook convert
```{r}
 library(rmd2jupyter)
 rmd2jupyter('rainfall_roc_auc_workflow.Rmd')
```
